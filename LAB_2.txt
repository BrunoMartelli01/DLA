Ho preso una implementazione del PPO e ho provato ad risolvere l'ambiente CarRacing-v3.
ho addestrato 4 modelli:
uno Vanilla PPO best_model_883.06_v.zip ( v sta per vanilla)
uno con un wrapper che modifica le reward in base alla velocità best_model_xxx.xx_s.zip (s sta per speed)
uno con il wrapper che modifica le reward in a se si trova sul tracciato oppure no best_model_xxx.xx_p.zip(p sta per penalty)
e infino uno che mette insieme entrambi i wrapper best_model_xxx.xx_sp.zip (sp sta per speed + penalty)


sono riuscito a ottenere un miglioramento delle performance minimo ma ho comunque modificato come si comporta il modello ad
esempio con il wrapper penalty il modello non taglia le curve, mentre con il wrapper speed il modello cerca di accelerare
di più.

l'allenalmento viene interrotto dopo 50 volte che il modello non si migliora
